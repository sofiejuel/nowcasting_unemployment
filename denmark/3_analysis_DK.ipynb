{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/statsmodels/compat/pandas.py:49: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  data_klasses = (pandas.Series, pandas.DataFrame, pandas.Panel)\n"
     ]
    }
   ],
   "source": [
    "# STANDARD PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm #https://pypi.org/project/tqdm/#ipython-jupyter-integ½ration\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "import platform\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "# SCRAPE PACKAGES\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from pytrends.request import TrendReq #pip install pytrends\n",
    "\n",
    "# MODEL PACKAGES\n",
    "    #SKLEARN\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis as x13\n",
    "from sklearn.metrics import max_error, r2_score\n",
    "\n",
    "\n",
    "# CUSTOM FUNCTIONS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "currentdir = os.path.dirname(os.path.realpath('analysis_DK'))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "\n",
    "from func import (chunks, reindex, global_id, term_list, time_corr_plot, rmse, time_variable_plot, find_highest_corr, test_train_split,\n",
    "                  bootstrap_all_windows, bootstrap_n_samples, bootstrap_sample, final_model, final_model_boot, ar_1, grid_bestpar, tuning_window, tuning_window_mp, tuning_window_bestpar,\n",
    "                  model_tuning, seasadj, seasadj_col_list, abs_percentage_change, add_poly_terms, create_interaction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create extra lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = abs_percentage_change(df_analysis,\n",
    "                                    var_abs_change =  ['target_actual', 'target_lag', 'target_12_lag'],\n",
    "                                    var_pct_change =  [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Capital','target_lag*ID_Central Denmark','target_lag*ID_North Denmark', \n",
    "                     'target_lag*ID_Southern Denmark','target_lag*ID_Zealand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning params for window: 100%|█████████████████████████████████████████████████████| 103/103 [00:00<00:00, 434.61it/s]\n"
     ]
    }
   ],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22869193252058517"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['best'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/baseline/results_baseline_ar1.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21441759587971718"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07, 0.06, 0.  , 0.01, 0.07])"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['y_pred_dict'][results_ols[1]['best'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(1) augmented with GT PCA1 + Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])\n",
    "df_PCA = pd.read_csv('OLD/df_PCA.csv', sep = ';', parse_dates=['date'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding GT PC1\n",
    "df_analysis = pd.merge(df_analysis, df_PCA[['ID', 'date', '0']], how = 'left', on =['ID', 'date']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = abs_percentage_change(df_analysis,\n",
    "                                    var_abs_change =  ['target_actual', 'target_lag'],\n",
    "                                    var_pct_change =  [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', '0'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Capital','target_lag*ID_Central Denmark','target_lag*ID_North Denmark', 'target_lag*ID_Southern Denmark','target_lag*ID_Zealand',\n",
    "                    '0', 'jobs']].copy() #'0*ID_Capital', '0*ID_Central Denmark', '0*ID_North Denmark', '0*ID_Southern Denmark', '0*ID_Zealand'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/model 1/ar1/results_ar1_augmented.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(2) with y_t-1 + y_t-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create extra lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_2_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = abs_percentage_change(df_analysis,\n",
    "                                    var_abs_change =  ['target_actual', 'target_lag', 'target_2_lag'],\n",
    "                                    var_pct_change =  [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_2_lag'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Capital','target_lag*ID_Central Denmark','target_lag*ID_North Denmark', \n",
    "                     'target_lag*ID_Southern Denmark','target_lag*ID_Zealand',\n",
    "                     'target_2_lag*ID_Capital','target_2_lag*ID_Central Denmark','target_2_lag*ID_North Denmark', \n",
    "                     'target_2_lag*ID_Southern Denmark','target_2_lag*ID_Zealand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning params for window: 100%|██████████| 113/113 [00:00<00:00, 392.50it/s]\n"
     ]
    }
   ],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.402367990774614"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['best'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ar1/results_ar2_five.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.228357789924281"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(2) with y_t-1 + y_t-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create extra lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = abs_percentage_change(df_analysis,\n",
    "                                    var_abs_change =  ['target_actual', 'target_lag', 'target_12_lag'],\n",
    "                                    var_pct_change =  [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Capital','target_lag*ID_Central Denmark','target_lag*ID_North Denmark', \n",
    "                     'target_lag*ID_Southern Denmark','target_lag*ID_Zealand',\n",
    "                     'target_12_lag*ID_Capital','target_12_lag*ID_Central Denmark','target_12_lag*ID_North Denmark', \n",
    "                     'target_12_lag*ID_Southern Denmark','target_12_lag*ID_Zealand']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning params for window:   0%|                                                                | 0/103 [00:00<?, ?it/s]\n",
      "Tuning params for window: 100%|█████████████████████████████████████████████████████| 103/103 [00:00<00:00, 624.24it/s]\n"
     ]
    }
   ],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14498275759551524"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['best'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/baseli/results_ar2_12_five.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10561074656657451"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Final models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## ML - Data and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/train data split\n",
    "\n",
    "Data must be split non-randomly as it needs to adhere to the underlying time structure. Two distinct approaches:\n",
    "\n",
    "1. Rolling window (fixed length)\n",
    "1. Expanding window (initial length that increases with each iteration)\n",
    "\n",
    "Each model must be run as a loop over the test/train splits. Thus, we will have multiple test/train splits for both rolling window and expanding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/y_dates.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_dates, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 10000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 8, num = 10000) #random.sample(list(np.logspace(-10,5, num = 15000)), k = 15000)\n",
    "n_components = list(np.arange(0.6, 0.95, 0.05).round(2))#[0.60, 0.70, 0.80, 0.90]\n",
    "params = [(alpha) for alpha in alphas]\n",
    "print('Number of param sets: '+ str(len(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 103/103 [1:32:53<00:00, 54.12s/it]\n"
     ]
    }
   ],
   "source": [
    "results_lasso_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_lasso_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_mp.pickle', 'rb') as handle:\n",
    "    results_lasso_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 253.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_lasso_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13734338307525912"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 10000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 8, num = 10000) #random.sample(list(np.logspace(-10,5, num = 15000)), k = 15000)\n",
    "n_components = list(np.arange(0.6, 0.95, 0.05).round(2))#[0.60, 0.70, 0.80, 0.90]\n",
    "params = [(alpha) for alpha in alphas]\n",
    "print('Number of param sets: '+ str(len(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 103/103 [2:04:21<00:00, 72.45s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_mp.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 283.75it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13259001697715023"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 200000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8,8, num = 10000) #2000\n",
    "n_components= [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "l1_ratio = list(np.arange(0.01,0.99,0.05)) ## 0.0 cannot be included due to a bug in the code\n",
    "params = [(alpha, ratio) for alpha in alphas for ratio in l1_ratio]\n",
    "print('Number of param sets: '+ str(len(params)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 103/103 [28:06:25<00:00, 982.39s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'elasticnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_mp.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 103/103 [00:00<00:00, 326.59it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1357748422538827"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## ML - Data and preprocessing - new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "#interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "#interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for var1 in interaction_1:\n",
    "#    for var2 in interaction_2:\n",
    "#        name = var1 + \"*\" + var2\n",
    "#        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3510\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "#range(1, X_train[1].shape[1] +1)\n",
    "# Number of trees in random forest\n",
    "n_estimators = [*range(50, 500, 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', \"sqrt\"] # Consider whether this should be set to 'auto as PCA should do its job'\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "# https://gdcoder.com/random-forest-regression-model-explained-in-depth-part-2-python-code-snippet-using-sklearn/\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split,\n",
    "     'min_samples_leaf':min_samples_leaf\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning params for window:  47%|████▋     | 48/103 [26:43:51<33:32:58, 2195.98s/it]"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_mp_no_int.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_mp_no_int.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'randomforest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_final_no_int.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:56<00:00,  1.71s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_shap.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_shap.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [00:18<00:00,  5.48it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_final_shap.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13276080889729835"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing data tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## No GT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
       "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
       "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
       "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
       "       'GT_DK_9', 'GT_DK_9_1_lag', 'ID_Central Denmark', 'ID_North Denmark',\n",
       "       'ID_Southern Denmark', 'ID_Zealand', 'M_10', 'M_11', 'M_12', 'M_2',\n",
       "       'M_3', 'M_4', 'M_5', 'M_6', 'M_7', 'M_8', 'M_9', 'date', 'jobs',\n",
       "       'jobs_3_lag', 'labour_force_share', 'mvu_lvu_share_pop', 'pop',\n",
       "       'sector_engineering_technology', 'sector_engineering_technology_3_lag',\n",
       "       'sector_industry_craft', 'sector_industry_craft_3_lag',\n",
       "       'sector_information_technology', 'sector_information_technology_3_lag',\n",
       "       'sector_management_staff', 'sector_management_staff_3_lag',\n",
       "       'sector_office_finance', 'sector_office_finance_3_lag',\n",
       "       'sector_social_health', 'sector_social_health_3_lag', 'sector_teaching',\n",
       "       'sector_teaching_3_lag', 'sector_trade_service',\n",
       "       'sector_trade_service_3_lag', 't', 'target_12_lag*ID_Capital',\n",
       "       'target_12_lag*ID_Central Denmark', 'target_12_lag*ID_North Denmark',\n",
       "       'target_12_lag*ID_Southern Denmark', 'target_12_lag*ID_Zealand',\n",
       "       'target_actual', 'target_lag*ID_Capital',\n",
       "       'target_lag*ID_Central Denmark', 'target_lag*ID_North Denmark',\n",
       "       'target_lag*ID_Southern Denmark', 'target_lag*ID_Zealand',\n",
       "       'w_ave_urban_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
    "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
    "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
    "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
    "       'GT_DK_9', 'GT_DK_9_1_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:45<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noGT.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noGT.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [00:15<00:00,  6.70it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_final_noGT.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13023529679233692"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## No JOBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
       "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
       "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
       "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
       "       'GT_DK_9', 'GT_DK_9_1_lag', 'ID_Central Denmark', 'ID_North Denmark',\n",
       "       'ID_Southern Denmark', 'ID_Zealand', 'M_10', 'M_11', 'M_12', 'M_2',\n",
       "       'M_3', 'M_4', 'M_5', 'M_6', 'M_7', 'M_8', 'M_9', 'date',\n",
       "       'labour_force_share', 'mvu_lvu_share_pop', 'pop', 't',\n",
       "       'target_12_lag*ID_Capital', 'target_12_lag*ID_Central Denmark',\n",
       "       'target_12_lag*ID_North Denmark', 'target_12_lag*ID_Southern Denmark',\n",
       "       'target_12_lag*ID_Zealand', 'target_actual', 'target_lag*ID_Capital',\n",
       "       'target_lag*ID_Central Denmark', 'target_lag*ID_North Denmark',\n",
       "       'target_lag*ID_Southern Denmark', 'target_lag*ID_Zealand',\n",
       "       'w_ave_urban_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['jobs', 'jobs_3_lag', 'sector_engineering_technology', 'sector_engineering_technology_3_lag',\n",
    "       'sector_industry_craft', 'sector_industry_craft_3_lag',\n",
    "       'sector_information_technology', 'sector_information_technology_3_lag',\n",
    "       'sector_management_staff', 'sector_management_staff_3_lag',\n",
    "       'sector_office_finance', 'sector_office_finance_3_lag',\n",
    "       'sector_social_health', 'sector_social_health_3_lag', 'sector_teaching',\n",
    "       'sector_teaching_3_lag', 'sector_trade_service',\n",
    "       'sector_trade_service_3_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:41<00:00,  1.57s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noJOB.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noJOB.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [00:16<00:00,  6.14it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_final_noJOB.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13738561634886023"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## No GT + JOBS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID_Central Denmark', 'ID_North Denmark', 'ID_Southern Denmark',\n",
       "       'ID_Zealand', 'M_10', 'M_11', 'M_12', 'M_2', 'M_3', 'M_4', 'M_5', 'M_6',\n",
       "       'M_7', 'M_8', 'M_9', 'date', 'labour_force_share', 'mvu_lvu_share_pop',\n",
       "       'pop', 't', 'target_12_lag*ID_Capital',\n",
       "       'target_12_lag*ID_Central Denmark', 'target_12_lag*ID_North Denmark',\n",
       "       'target_12_lag*ID_Southern Denmark', 'target_12_lag*ID_Zealand',\n",
       "       'target_actual', 'target_lag*ID_Capital',\n",
       "       'target_lag*ID_Central Denmark', 'target_lag*ID_North Denmark',\n",
       "       'target_lag*ID_Southern Denmark', 'target_lag*ID_Zealand',\n",
       "       'w_ave_urban_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['jobs', 'jobs_3_lag', 'sector_engineering_technology', 'sector_engineering_technology_3_lag',\n",
    "       'sector_industry_craft', 'sector_industry_craft_3_lag',\n",
    "       'sector_information_technology', 'sector_information_technology_3_lag',\n",
    "       'sector_management_staff', 'sector_management_staff_3_lag',\n",
    "       'sector_office_finance', 'sector_office_finance_3_lag',\n",
    "       'sector_social_health', 'sector_social_health_3_lag', 'sector_teaching',\n",
    "       'sector_teaching_3_lag', 'sector_trade_service',\n",
    "       'sector_trade_service_3_lag', 'GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
    "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
    "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
    "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
    "       'GT_DK_9', 'GT_DK_9_1_lag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:21<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noGTJOB.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_noGTJOB.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [00:14<00:00,  7.08it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_final_noGTJOB.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12650599499407195"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## No GT + JOBS + CONTROLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading adjusted df_analysis\n",
    "df_analysis = pd.read_csv('data/df_DK.csv', sep = ',', parse_dates = ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_sales_communication', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping socioeconomic index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['w_ave_socio_index', 'target_lag'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_12_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_3m_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology',\n",
    "                   'sector_management_staff', 'sector_trade_service', 'sector_industry_craft',\n",
    "                   'sector_teaching', 'sector_office_finance', 'sector_social_health']\n",
    "\n",
    "for colname in columns_3m_lag:\n",
    "    df_analysis[str(colname + '_3_lag')] = df_analysis.groupby(['ID'])[colname].shift(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_DK_1', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_8', 'GT_DK_11', 'GT_DK_13', 'GT_DK_15', 'GT_DK_16', 'GT_DK_18', 'GT_DK_19', 'GT_DK_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1m_lag = ['GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_7', 'GT_DK_9', 'GT_DK_10', 'GT_DK_12', 'GT_DK_14', 'GT_DK_17', 'GT_DK_20']\n",
    "for colname in columns_1m_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['month'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','M'], prefix_sep='_', columns=['ID', 'month']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-03-01 00:00:00')"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_12_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Capital', 'M_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
       "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
       "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
       "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
       "       'GT_DK_9', 'GT_DK_9_1_lag', 'ID_Central Denmark', 'ID_North Denmark',\n",
       "       'ID_Southern Denmark', 'ID_Zealand', 'M_10', 'M_11', 'M_12', 'M_2',\n",
       "       'M_3', 'M_4', 'M_5', 'M_6', 'M_7', 'M_8', 'M_9', 'date', 'jobs',\n",
       "       'jobs_3_lag', 'labour_force_share', 'mvu_lvu_share_pop', 'pop',\n",
       "       'sector_engineering_technology', 'sector_engineering_technology_3_lag',\n",
       "       'sector_industry_craft', 'sector_industry_craft_3_lag',\n",
       "       'sector_information_technology', 'sector_information_technology_3_lag',\n",
       "       'sector_management_staff', 'sector_management_staff_3_lag',\n",
       "       'sector_office_finance', 'sector_office_finance_3_lag',\n",
       "       'sector_social_health', 'sector_social_health_3_lag', 'sector_teaching',\n",
       "       'sector_teaching_3_lag', 'sector_trade_service',\n",
       "       'sector_trade_service_3_lag', 't', 'target_12_lag*ID_Capital',\n",
       "       'target_12_lag*ID_Central Denmark', 'target_12_lag*ID_North Denmark',\n",
       "       'target_12_lag*ID_Southern Denmark', 'target_12_lag*ID_Zealand',\n",
       "       'target_actual', 'target_lag*ID_Capital',\n",
       "       'target_lag*ID_Central Denmark', 'target_lag*ID_North Denmark',\n",
       "       'target_lag*ID_Southern Denmark', 'target_lag*ID_Zealand',\n",
       "       'w_ave_urban_index'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['jobs', 'jobs_3_lag', 'sector_engineering_technology', 'sector_engineering_technology_3_lag',\n",
    "       'sector_industry_craft', 'sector_industry_craft_3_lag',\n",
    "       'sector_information_technology', 'sector_information_technology_3_lag',\n",
    "       'sector_management_staff', 'sector_management_staff_3_lag',\n",
    "       'sector_office_finance', 'sector_office_finance_3_lag',\n",
    "       'sector_social_health', 'sector_social_health_3_lag', 'sector_teaching',\n",
    "       'sector_teaching_3_lag', 'sector_trade_service',\n",
    "       'sector_trade_service_3_lag', 'GT_DK_0', 'GT_DK_0_1_lag', 'GT_DK_10', 'GT_DK_10_1_lag', 'GT_DK_12',\n",
    "       'GT_DK_12_1_lag', 'GT_DK_14', 'GT_DK_14_1_lag', 'GT_DK_17',\n",
    "       'GT_DK_17_1_lag', 'GT_DK_2', 'GT_DK_20', 'GT_DK_20_1_lag',\n",
    "       'GT_DK_2_1_lag', 'GT_DK_3', 'GT_DK_3_1_lag', 'GT_DK_7', 'GT_DK_7_1_lag',\n",
    "       'GT_DK_9', 'GT_DK_9_1_lag', 'w_ave_urban_index', 'labour_force_share', 'mvu_lvu_share_pop', 'pop',\n",
    "            'M_10', 'M_11', 'M_12', 'M_2', 'M_3', 'M_4', 'M_5', 'M_6',\n",
    "       'M_7', 'M_8', 'M_9', 't', 'ID_Central Denmark', 'ID_North Denmark', 'ID_Southern Denmark',\n",
    "       'ID_Zealand']\n",
    "# 'target_12_lag*ID_Central Denmark', 'target_12_lag*ID_North Denmark', 'target_12_lag*ID_Southern Denmark', 'target_12_lag*ID_Zealand',    'target_12_lag*ID_Capital'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 35\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [02:03<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_ONLYLAG.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_ONLYLAG.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 103/103 [00:10<00:00, 10.25it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/no_data/xgboost/results_final_ONLYLAG.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13723135096243036"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21281800480529658"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
