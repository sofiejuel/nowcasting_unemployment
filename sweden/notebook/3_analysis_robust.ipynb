{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STANDARD PACKAGES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from tqdm import tqdm #https://pypi.org/project/tqdm/#ipython-jupyter-integ½ration\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import itertools\n",
    "import warnings\n",
    "import platform\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "# SCRAPE PACKAGES\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# from pytrends.request import TrendReq #pip install pytrends\n",
    "\n",
    "# MODEL PACKAGES\n",
    "    #SKLEARN\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.x13 import x13_arima_analysis as x13\n",
    "\n",
    "# CUSTOM FUNCTIONS\n",
    "\n",
    "import os\n",
    "import sys\n",
    "currentdir = os.path.dirname(os.path.realpath('analysis_DK'))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.append(parentdir)\n",
    "\n",
    "from func import (chunks, reindex, global_id, term_list, time_corr_plot, rmse, time_variable_plot, find_highest_corr, test_train_split, test_train_split_Q,\n",
    "                  bootstrap_all_windows, bootstrap_n_samples, bootstrap_sample, final_model, final_model_boot, ar_1, grid_bestpar, tuning_window, tuning_window_mp, tuning_window_bestpar,\n",
    "                  model_tuning, seasadj, seasadj_col_list, abs_percentage_change, add_poly_terms, create_interaction, GT_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Final model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('data/descriptive/df_descriptive.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.country == 'SE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[~(df_analysis.ID == 'Jämtland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['GT_NO_0', 'GT_NO_1', 'GT_NO_2', 'GT_NO_6', 'GT_NO_7', 'GT_NO_8', 'GT_NO_10', 'GT_NO_11',  \n",
    "                                'GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_7', 'GT_DK_8',\n",
    "                                'GT_DK_9', 'GT_DK_10', 'GT_DK_11', 'GT_DK_12', 'GT_DK_13', 'GT_DK_14',\n",
    "                                'GT_DK_15', 'GT_DK_16', 'GT_DK_17', 'GT_DK_18', 'GT_DK_19', 'GT_DK_20', 'GT_DK_21',\n",
    "                                'country'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.to_csv('data/descriptive/df_analysis.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "Blekinge             154134.0\n",
       "Dalarna              278771.0\n",
       "Gotland               57479.0\n",
       "Gävleborg            278814.0\n",
       "Halland              305064.0\n",
       "Jönköping            341693.0\n",
       "Kalmar               235971.0\n",
       "Kronoberg            187031.0\n",
       "Norrbotten           249896.0\n",
       "Skåne               1262614.0\n",
       "Stockholm           2114130.0\n",
       "Södermanland         275851.0\n",
       "Uppsala              342115.0\n",
       "Värmland             274999.0\n",
       "Västerbotten         261286.0\n",
       "Västernorrland       243523.0\n",
       "Västmanland          257907.0\n",
       "Västra Götalands    1606265.0\n",
       "Örebro               284760.0\n",
       "Östergötland         435579.0\n",
       "Name: population, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.groupby(['ID'])['population'].mean().round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 11\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Blekinge', 'target_lag*ID_Dalarna', 'target_lag*ID_Gotland', 'target_lag*ID_Gävleborg', \n",
    "                     'target_lag*ID_Halland', 'target_lag*ID_Jönköping', 'target_lag*ID_Kalmar', \n",
    "                     'target_lag*ID_Kronoberg', 'target_lag*ID_Norrbotten', 'target_lag*ID_Skåne', 'target_lag*ID_Stockholm', \n",
    "                     'target_lag*ID_Södermanland', 'target_lag*ID_Uppsala', 'target_lag*ID_Värmland', 'target_lag*ID_Västerbotten', \n",
    "                     'target_lag*ID_Västernorrland', 'target_lag*ID_Västmanland', 'target_lag*ID_Västra Götalands', 'target_lag*ID_Örebro', \n",
    "                     'target_lag*ID_Östergötland']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Quarter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_ar['date'] = df_ar.set_index('date').index.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2007Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ar['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2019Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ar['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset for period to match DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_ar[df_ar.date >= '01-01-2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning params for window:   0%|          | 0/35 [00:00<?, ?it/s]\u001b[A\n",
      "Tuning params for window: 100%|██████████| 35/35 [00:00<00:00, 1191.33it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2138739638034914"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['best'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/baseline/results_ar1.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3189239646182216"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14,  0.23, -0.91,  0.26,  0.08,  0.15,  0.18,  0.25, -0.17,\n",
       "        0.22,  0.14,  0.27,  0.2 ,  0.2 , -0.11,  0.16,  0.04,  0.13,\n",
       "        0.03,  0.32])"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['y_pred_dict'][results_ols[1]['best'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Baseline -  AR(2) with y_t-1 + y_t-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('data/descriptive/df_descriptive.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.country == 'SE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[~(df_analysis.ID == 'Jämtland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['GT_NO_0', 'GT_NO_1', 'GT_NO_2', 'GT_NO_6', 'GT_NO_7', 'GT_NO_8', 'GT_NO_10', 'GT_NO_11',  \n",
    "                                'GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_7', 'GT_DK_8',\n",
    "                                'GT_DK_9', 'GT_DK_10', 'GT_DK_11', 'GT_DK_12', 'GT_DK_13', 'GT_DK_14',\n",
    "                                'GT_DK_15', 'GT_DK_16', 'GT_DK_17', 'GT_DK_18', 'GT_DK_19', 'GT_DK_20', 'GT_DK_21',\n",
    "                                'country'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_4_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID'], prefix_sep='_', columns=['ID']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_4_lag'] \n",
    "\n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 11\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True\n",
    "params = []\n",
    "n_components = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_analysis[['date', 'target_actual', \n",
    "                     'target_lag*ID_Blekinge', 'target_lag*ID_Dalarna', 'target_lag*ID_Gotland', 'target_lag*ID_Gävleborg', \n",
    "                     'target_lag*ID_Halland', 'target_lag*ID_Jönköping', 'target_lag*ID_Kalmar', \n",
    "                     'target_lag*ID_Kronoberg', 'target_lag*ID_Norrbotten', 'target_lag*ID_Skåne', 'target_lag*ID_Stockholm', \n",
    "                     'target_lag*ID_Södermanland', 'target_lag*ID_Uppsala', 'target_lag*ID_Värmland', 'target_lag*ID_Västerbotten', \n",
    "                     'target_lag*ID_Västernorrland', 'target_lag*ID_Västmanland', 'target_lag*ID_Västra Götalands', 'target_lag*ID_Örebro', \n",
    "                     'target_lag*ID_Östergötland',\n",
    "                    'target_4_lag*ID_Blekinge', 'target_4_lag*ID_Dalarna', 'target_4_lag*ID_Gotland',\n",
    "                     'target_4_lag*ID_Gävleborg', 'target_4_lag*ID_Halland', 'target_4_lag*ID_Jönköping', \n",
    "                     'target_4_lag*ID_Kalmar', 'target_4_lag*ID_Kronoberg', 'target_4_lag*ID_Norrbotten', 'target_4_lag*ID_Skåne', \n",
    "                     'target_4_lag*ID_Stockholm', 'target_4_lag*ID_Södermanland', 'target_4_lag*ID_Uppsala', 'target_4_lag*ID_Värmland',\n",
    "                     'target_4_lag*ID_Västerbotten', 'target_4_lag*ID_Västernorrland', 'target_4_lag*ID_Västmanland', \n",
    "                     'target_4_lag*ID_Västra Götalands', 'target_4_lag*ID_Örebro', 'target_4_lag*ID_Östergötland']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Quarter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_ar['date'] = df_ar.set_index('date').index.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2008Q2', 'Q-DEC')"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ar['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2019Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ar['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset for period to match DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_ar[df_ar.date >= '01-01-2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_ar, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatting val and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimating with OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Tuning params for window:   0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "Tuning params for window: 100%|██████████| 34/34 [00:00<00:00, 583.08it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "results_ols= tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, params = params, n_components = n_components, model_str = 'ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0421324292046568"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['best'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/baseline/results_ar_year_lag.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_ols, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for key in results_ols.keys():\n",
    "    temp.append(results_ols[key]['best'][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2295557185457329"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8 ,  0.34, -0.49,  0.19, -0.52,  0.43, -0.85,  0.04,  0.4 ,\n",
       "        0.25,  0.3 , -0.02,  0.54, -2.75,  1.44,  0.26, -0.86,  0.28,\n",
       "        0.42,  1.65])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_ols[1]['y_pred_dict'][results_ols[1]['best'][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## ML - Data and preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('data/descriptive/df_descriptive.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.country == 'SE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[~(df_analysis.ID == 'Jämtland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['GT_NO_0', 'GT_NO_1', 'GT_NO_2', 'GT_NO_6', 'GT_NO_7', 'GT_NO_8', 'GT_NO_10', 'GT_NO_11',  \n",
    "                                'GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_7', 'GT_DK_8',\n",
    "                                'GT_DK_9', 'GT_DK_10', 'GT_DK_11', 'GT_DK_12', 'GT_DK_13', 'GT_DK_14',\n",
    "                                'GT_DK_15', 'GT_DK_16', 'GT_DK_17', 'GT_DK_18', 'GT_DK_19', 'GT_DK_20', 'GT_DK_21',\n",
    "                                'country'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'target_actual', 'ID', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other', 'population', 'high_edu_share', 'labour_force_share', 'urban_share', 'GT_DK_1', 'GT_SE_0', 'GT_SE_1', 'GT_SE_2', 'GT_SE_5', 'GT_SE_6', 'GT_SE_7', 'GT_SE_8', 'GT_SE_9', 'GT_SE_10', 'GT_SE_11']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2007-01-01 00:00:00')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-07-01 00:00:00')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criteria = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_management_staff', 'sector_trade_service', 'sector_sales_communication', 'sector_teaching', 'sector_social_health', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! As this is Quarterly data one year lag is 4 quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_4_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month (1 quarter) lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1q_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_industry_craft','sector_office_finance']\n",
    "\n",
    "for colname in columns_1q_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_SE_6', 'GT_SE_8', 'GT_SE_10', 'GT_SE_11', 'GT_DK_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1q_lag = ['GT_SE_0', 'GT_SE_1', 'GT_SE_2', 'GT_SE_5', 'GT_SE_7', 'GT_SE_9']\n",
    "for colname in columns_1q_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['quarter'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','Q'], prefix_sep='_', columns=['ID', 'quarter']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-07-01 00:00:00')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-04-01 00:00:00')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant interaction variables\n",
    "interaction_1 = ['target_lag', 'target_4_lag'] \n",
    "# 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# get list of all ID area \n",
    "interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var1 in interaction_1:\n",
    "    for var2 in interaction_2:\n",
    "        name = var1 + \"*\" + var2\n",
    "        df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Blekinge', 'Q_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Quarter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['date'] = df_analysis.set_index('date').index.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2008Q2', 'Q-DEC')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2019Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset for period to match DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.date >= '01-01-2008']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GT_SE_0', 'GT_SE_0_1_lag', 'GT_SE_1', 'GT_SE_1_1_lag', 'GT_SE_2',\n",
       "       'GT_SE_2_1_lag', 'GT_SE_5', 'GT_SE_5_1_lag', 'GT_SE_7', 'GT_SE_7_1_lag',\n",
       "       'GT_SE_9', 'GT_SE_9_1_lag', 'ID_Dalarna', 'ID_Gotland', 'ID_Gävleborg',\n",
       "       'ID_Halland', 'ID_Jönköping', 'ID_Kalmar', 'ID_Kronoberg',\n",
       "       'ID_Norrbotten', 'ID_Skåne', 'ID_Stockholm', 'ID_Södermanland',\n",
       "       'ID_Uppsala', 'ID_Värmland', 'ID_Västerbotten', 'ID_Västernorrland',\n",
       "       'ID_Västmanland', 'ID_Västra Götalands', 'ID_Örebro', 'ID_Östergötland',\n",
       "       'Q_10', 'Q_4', 'Q_7', 'date', 'high_edu_share', 'jobs', 'jobs_1_lag',\n",
       "       'labour_force_share', 'population', 'sector_engineering_technology',\n",
       "       'sector_engineering_technology_1_lag', 'sector_industry_craft',\n",
       "       'sector_industry_craft_1_lag', 'sector_information_technology',\n",
       "       'sector_information_technology_1_lag', 'sector_office_finance',\n",
       "       'sector_office_finance_1_lag', 't', 'target_4_lag*ID_Blekinge',\n",
       "       'target_4_lag*ID_Dalarna', 'target_4_lag*ID_Gotland',\n",
       "       'target_4_lag*ID_Gävleborg', 'target_4_lag*ID_Halland',\n",
       "       'target_4_lag*ID_Jönköping', 'target_4_lag*ID_Kalmar',\n",
       "       'target_4_lag*ID_Kronoberg', 'target_4_lag*ID_Norrbotten',\n",
       "       'target_4_lag*ID_Skåne', 'target_4_lag*ID_Stockholm',\n",
       "       'target_4_lag*ID_Södermanland', 'target_4_lag*ID_Uppsala',\n",
       "       'target_4_lag*ID_Värmland', 'target_4_lag*ID_Västerbotten',\n",
       "       'target_4_lag*ID_Västernorrland', 'target_4_lag*ID_Västmanland',\n",
       "       'target_4_lag*ID_Västra Götalands', 'target_4_lag*ID_Örebro',\n",
       "       'target_4_lag*ID_Östergötland', 'target_actual',\n",
       "       'target_lag*ID_Blekinge', 'target_lag*ID_Dalarna',\n",
       "       'target_lag*ID_Gotland', 'target_lag*ID_Gävleborg',\n",
       "       'target_lag*ID_Halland', 'target_lag*ID_Jönköping',\n",
       "       'target_lag*ID_Kalmar', 'target_lag*ID_Kronoberg',\n",
       "       'target_lag*ID_Norrbotten', 'target_lag*ID_Skåne',\n",
       "       'target_lag*ID_Stockholm', 'target_lag*ID_Södermanland',\n",
       "       'target_lag*ID_Uppsala', 'target_lag*ID_Värmland',\n",
       "       'target_lag*ID_Västerbotten', 'target_lag*ID_Västernorrland',\n",
       "       'target_lag*ID_Västmanland', 'target_lag*ID_Västra Götalands',\n",
       "       'target_lag*ID_Örebro', 'target_lag*ID_Östergötland', 'urban_share'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 11\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test/train data split\n",
    "\n",
    "Data must be split non-randomly as it needs to adhere to the underlying time structure. Two distinct approaches:\n",
    "\n",
    "1. Rolling window (fixed length)\n",
    "1. Expanding window (initial length that increases with each iteration)\n",
    "\n",
    "Each model must be run as a loop over the test/train splits. Thus, we will have multiple test/train splits for both rolling window and expanding window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/y_dates.pickle', 'wb') as handle:\n",
    "    pickle.dump(y_dates, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 10000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 8, num = 10000) #random.sample(list(np.logspace(-10,5, num = 15000)), k = 15000)\n",
    "n_components = list(np.arange(0.6, 0.95, 0.05).round(2))#[0.60, 0.70, 0.80, 0.90]\n",
    "params = [(alpha) for alpha in alphas]\n",
    "print('Number of param sets: '+ str(len(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/34 [01:23<45:43, 83.15s/it]\u001b[A\n",
      "  6%|▌         | 2/34 [02:46<44:21, 83.16s/it]\u001b[A\n",
      "  9%|▉         | 3/34 [03:58<41:19, 79.98s/it]\u001b[A\n",
      " 12%|█▏        | 4/34 [05:18<39:59, 79.99s/it]\u001b[A\n",
      " 15%|█▍        | 5/34 [06:45<39:40, 82.08s/it]\u001b[A\n",
      " 18%|█▊        | 6/34 [08:09<38:34, 82.65s/it]\u001b[A\n",
      " 21%|██        | 7/34 [09:40<38:19, 85.16s/it]\u001b[A\n",
      " 24%|██▎       | 8/34 [10:51<35:02, 80.87s/it]\u001b[A\n",
      " 26%|██▋       | 9/34 [12:11<33:30, 80.43s/it]\u001b[A\n",
      " 29%|██▉       | 10/34 [13:24<31:18, 78.28s/it]\u001b[A\n",
      " 32%|███▏      | 11/34 [14:31<28:45, 75.04s/it]\u001b[A\n",
      " 35%|███▌      | 12/34 [15:40<26:47, 73.06s/it]\u001b[A\n",
      " 38%|███▊      | 13/34 [16:54<25:39, 73.31s/it]\u001b[A\n",
      " 41%|████      | 14/34 [18:01<23:51, 71.59s/it]\u001b[A\n",
      " 44%|████▍     | 15/34 [19:08<22:13, 70.17s/it]\u001b[A\n",
      " 47%|████▋     | 16/34 [20:15<20:45, 69.18s/it]\u001b[A\n",
      " 50%|█████     | 17/34 [21:22<19:23, 68.44s/it]\u001b[A\n",
      " 53%|█████▎    | 18/34 [22:29<18:10, 68.18s/it]\u001b[A\n",
      " 56%|█████▌    | 19/34 [23:36<16:55, 67.71s/it]\u001b[A\n",
      " 59%|█████▉    | 20/34 [24:44<15:47, 67.70s/it]\u001b[A\n",
      " 62%|██████▏   | 21/34 [26:08<15:44, 72.63s/it]\u001b[A\n",
      " 65%|██████▍   | 22/34 [27:21<14:32, 72.70s/it]\u001b[A\n",
      " 68%|██████▊   | 23/34 [28:28<13:02, 71.09s/it]\u001b[A\n",
      " 71%|███████   | 24/34 [30:05<13:08, 78.86s/it]\u001b[A\n",
      " 74%|███████▎  | 25/34 [31:16<11:29, 76.56s/it]\u001b[A\n",
      " 76%|███████▋  | 26/34 [32:30<10:06, 75.84s/it]\u001b[A\n",
      " 79%|███████▉  | 27/34 [33:41<08:41, 74.46s/it]\u001b[A\n",
      " 82%|████████▏ | 28/34 [34:54<07:22, 73.80s/it]\u001b[A\n",
      " 85%|████████▌ | 29/34 [36:03<06:02, 72.58s/it]\u001b[A\n",
      " 88%|████████▊ | 30/34 [37:18<04:52, 73.15s/it]\u001b[A\n",
      " 91%|█████████ | 31/34 [38:35<03:43, 74.41s/it]\u001b[A\n",
      " 94%|█████████▍| 32/34 [39:58<02:33, 76.81s/it]\u001b[A\n",
      " 97%|█████████▋| 33/34 [41:12<01:16, 76.10s/it]\u001b[A\n",
      "100%|██████████| 34/34 [42:28<00:00, 75.91s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "results_lasso_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_lasso_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_mp.pickle', 'rb') as handle:\n",
    "    results_lasso_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      " 68%|██████▊   | 23/34 [00:00<00:00, 223.86it/s]\u001b[A\n",
      "100%|██████████| 34/34 [00:00<00:00, 215.27it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_lasso_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'lasso')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/lasso/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2144491919547746"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 10000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8, 8, num = 10000) #random.sample(list(np.logspace(-10,5, num = 15000)), k = 15000)\n",
    "n_components = list(np.arange(0.6, 0.95, 0.05).round(2))#[0.60, 0.70, 0.80, 0.90]\n",
    "params = [(alpha) for alpha in alphas]\n",
    "print('Number of param sets: '+ str(len(params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      "  3%|▎         | 1/34 [01:14<40:51, 74.28s/it]\u001b[A\n",
      "  6%|▌         | 2/34 [02:25<39:12, 73.50s/it]\u001b[A\n",
      "  9%|▉         | 3/34 [03:38<37:50, 73.25s/it]\u001b[A\n",
      " 12%|█▏        | 4/34 [04:52<36:44, 73.48s/it]\u001b[A\n",
      " 15%|█▍        | 5/34 [06:04<35:19, 73.07s/it]\u001b[A\n",
      " 18%|█▊        | 6/34 [07:19<34:23, 73.68s/it]\u001b[A\n",
      " 21%|██        | 7/34 [08:47<34:58, 77.72s/it]\u001b[A\n",
      " 24%|██▎       | 8/34 [09:59<32:56, 76.01s/it]\u001b[A\n",
      " 26%|██▋       | 9/34 [11:11<31:14, 74.99s/it]\u001b[A\n",
      " 29%|██▉       | 10/34 [12:29<30:21, 75.91s/it]\u001b[A\n",
      " 32%|███▏      | 11/34 [13:39<28:23, 74.05s/it]\u001b[A\n",
      " 35%|███▌      | 12/34 [14:51<26:57, 73.53s/it]\u001b[A\n",
      " 38%|███▊      | 13/34 [16:04<25:38, 73.27s/it]\u001b[A\n",
      " 41%|████      | 14/34 [17:13<23:59, 71.95s/it]\u001b[A\n",
      " 44%|████▍     | 15/34 [18:25<22:49, 72.06s/it]\u001b[A\n",
      " 47%|████▋     | 16/34 [19:39<21:48, 72.68s/it]\u001b[A\n",
      " 50%|█████     | 17/34 [20:50<20:25, 72.08s/it]\u001b[A\n",
      " 53%|█████▎    | 18/34 [21:59<18:59, 71.20s/it]\u001b[A\n",
      " 56%|█████▌    | 19/34 [23:10<17:48, 71.22s/it]\u001b[A\n",
      " 59%|█████▉    | 20/34 [24:20<16:32, 70.91s/it]\u001b[A\n",
      " 62%|██████▏   | 21/34 [25:36<15:40, 72.33s/it]\u001b[A\n",
      " 65%|██████▍   | 22/34 [26:47<14:24, 72.04s/it]\u001b[A\n",
      " 68%|██████▊   | 23/34 [27:54<12:54, 70.39s/it]\u001b[A\n",
      " 71%|███████   | 24/34 [29:11<12:04, 72.42s/it]\u001b[A\n",
      " 74%|███████▎  | 25/34 [30:27<11:00, 73.37s/it]\u001b[A\n",
      " 76%|███████▋  | 26/34 [31:36<09:36, 72.07s/it]\u001b[A\n",
      " 79%|███████▉  | 27/34 [32:42<08:11, 70.28s/it]\u001b[A\n",
      " 82%|████████▏ | 28/34 [33:48<06:53, 68.94s/it]\u001b[A\n",
      " 85%|████████▌ | 29/34 [34:52<05:37, 67.55s/it]\u001b[A\n",
      " 88%|████████▊ | 30/34 [35:57<04:27, 66.78s/it]\u001b[A\n",
      " 91%|█████████ | 31/34 [37:04<03:20, 66.73s/it]\u001b[A\n",
      " 94%|█████████▍| 32/34 [38:10<02:13, 66.58s/it]\u001b[A\n",
      " 97%|█████████▋| 33/34 [39:24<01:08, 68.75s/it]\u001b[A\n",
      "100%|██████████| 34/34 [40:35<00:00, 69.50s/it]\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_mp.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\n",
      " 56%|█████▌    | 19/34 [00:00<00:00, 182.34it/s]\u001b[A\n",
      "100%|██████████| 34/34 [00:00<00:00, 180.81it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'ridge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/ridge/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2349172324583901"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of param sets: 200000\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-8,8, num = 10000) #2000\n",
    "n_components= [0.60, 0.65, 0.70, 0.75, 0.80, 0.85, 0.90]\n",
    "l1_ratio = list(np.arange(0.01,0.99,0.05)) ## 0.0 cannot be included due to a bug in the code\n",
    "params = [(alpha, ratio) for alpha in alphas for ratio in l1_ratio]\n",
    "print('Number of param sets: '+ str(len(params)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 1/34 [24:49<13:39:07, 1489.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 2/34 [46:39<12:45:43, 1435.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 3/34 [1:08:08<11:59:00, 1391.62s/it]\u001b[A\u001b[A\n",
      "\n",
      " 12%|█▏        | 4/34 [1:29:37<11:20:26, 1360.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 15%|█▍        | 5/34 [1:51:19<10:49:08, 1343.07s/it]\u001b[A\u001b[A\n",
      "\n",
      " 18%|█▊        | 6/34 [2:13:01<10:21:00, 1330.73s/it]\u001b[A\u001b[A\n",
      "\n",
      " 21%|██        | 7/34 [2:34:39<9:54:22, 1320.83s/it] \u001b[A\u001b[A\n",
      "\n",
      " 24%|██▎       | 8/34 [2:56:19<9:29:41, 1314.68s/it]\u001b[A\u001b[A\n",
      "\n",
      " 26%|██▋       | 9/34 [3:18:05<9:06:41, 1312.05s/it]\u001b[A\u001b[A\n",
      "\n",
      " 29%|██▉       | 10/34 [3:39:41<8:42:52, 1307.17s/it]\u001b[A\u001b[A\n",
      "\n",
      " 32%|███▏      | 11/34 [4:01:23<8:20:35, 1305.89s/it]\u001b[A\u001b[A\n",
      "\n",
      " 35%|███▌      | 12/34 [4:23:06<7:58:24, 1304.76s/it]\u001b[A\u001b[A\n",
      "\n",
      " 38%|███▊      | 13/34 [4:44:39<7:35:30, 1301.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 41%|████      | 14/34 [5:06:17<7:13:27, 1300.35s/it]\u001b[A\u001b[A\n",
      "\n",
      " 44%|████▍     | 15/34 [5:27:51<6:51:08, 1298.36s/it]\u001b[A\u001b[A\n",
      "\n",
      " 47%|████▋     | 16/34 [5:49:23<6:28:57, 1296.53s/it]\u001b[A\u001b[A\n",
      "\n",
      " 50%|█████     | 17/34 [6:10:58<6:07:15, 1296.19s/it]\u001b[A\u001b[A\n",
      "\n",
      " 53%|█████▎    | 18/34 [6:32:37<5:45:51, 1296.98s/it]\u001b[A\u001b[A\n",
      "\n",
      " 56%|█████▌    | 19/34 [6:54:14<5:24:11, 1296.78s/it]\u001b[A\u001b[A\n",
      "\n",
      " 59%|█████▉    | 20/34 [7:15:49<5:02:27, 1296.23s/it]\u001b[A\u001b[A\n",
      "\n",
      " 62%|██████▏   | 21/34 [7:37:22<4:40:38, 1295.29s/it]\u001b[A\u001b[A\n",
      "\n",
      " 65%|██████▍   | 22/34 [7:58:58<4:19:06, 1295.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 68%|██████▊   | 23/34 [8:20:40<3:57:52, 1297.51s/it]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 24/34 [8:42:21<3:36:24, 1298.46s/it]\u001b[A\u001b[A\n",
      "\n",
      " 74%|███████▎  | 25/34 [9:03:56<3:14:37, 1297.45s/it]\u001b[A\u001b[A\n",
      "\n",
      " 76%|███████▋  | 26/34 [9:25:33<2:52:59, 1297.41s/it]\u001b[A\u001b[A\n",
      "\n",
      " 79%|███████▉  | 27/34 [9:47:16<2:31:34, 1299.16s/it]\u001b[A\u001b[A\n",
      "\n",
      " 82%|████████▏ | 28/34 [10:09:03<2:10:08, 1301.40s/it]\u001b[A\u001b[A\n",
      "\n",
      " 85%|████████▌ | 29/34 [10:30:37<1:48:15, 1299.14s/it]\u001b[A\u001b[A\n",
      "\n",
      " 88%|████████▊ | 30/34 [10:52:15<1:26:35, 1298.86s/it]\u001b[A\u001b[A\n",
      "\n",
      " 91%|█████████ | 31/34 [11:13:49<1:04:52, 1297.55s/it]\u001b[A\u001b[A\n",
      "\n",
      " 94%|█████████▍| 32/34 [11:35:25<43:14, 1297.09s/it]  \u001b[A\u001b[A\n",
      "\n",
      " 97%|█████████▋| 33/34 [11:56:59<21:36, 1296.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 34/34 [12:18:36<00:00, 1296.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'elasticnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_mp.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_mp.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/34 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      " 71%|███████   | 24/34 [00:00<00:00, 236.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "100%|██████████| 34/34 [00:00<00:00, 236.15it/s]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'elasticnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/elastic/results_final.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2187329444035426"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## ML - Data and preprocessing - tree-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data frame with adjusted here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('data/descriptive/df_descriptive.csv', parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.country == 'SE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[~(df_analysis.ID == 'Jämtland')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['GT_NO_0', 'GT_NO_1', 'GT_NO_2', 'GT_NO_6', 'GT_NO_7', 'GT_NO_8', 'GT_NO_10', 'GT_NO_11',  \n",
    "                                'GT_DK_0', 'GT_DK_2', 'GT_DK_3', 'GT_DK_4', 'GT_DK_5', 'GT_DK_6', 'GT_DK_7', 'GT_DK_8',\n",
    "                                'GT_DK_9', 'GT_DK_10', 'GT_DK_11', 'GT_DK_12', 'GT_DK_13', 'GT_DK_14',\n",
    "                                'GT_DK_15', 'GT_DK_16', 'GT_DK_17', 'GT_DK_18', 'GT_DK_19', 'GT_DK_20', 'GT_DK_21',\n",
    "                                'country'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'target_actual', 'ID', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other', 'population', 'high_edu_share', 'labour_force_share', 'urban_share', 'GT_DK_1', 'GT_SE_0', 'GT_SE_1', 'GT_SE_2', 'GT_SE_5', 'GT_SE_6', 'GT_SE_7', 'GT_SE_8', 'GT_SE_9', 'GT_SE_10', 'GT_SE_11']\n"
     ]
    }
   ],
   "source": [
    "print(list(df_analysis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2007-01-01 00:00:00')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-07-01 00:00:00')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial preprocessing and feature construction\n",
    "\n",
    "- Create dummies \n",
    "- Create interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall monthly time trend variable, $t=1,2...,T$ within `ID` variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WSCHUPPLI\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#Temp container\n",
    "temp = {}\n",
    "\n",
    "for i in df_analysis['ID'].unique():\n",
    "    temp[i] = df_analysis[df_analysis['ID']==i]\n",
    "    temp[i]['t'] = range(1, len(temp[i]['ID'])+1)\n",
    "\n",
    "#Concatting the df's\n",
    "temp = pd.concat(temp, ignore_index=True, sort = False)\n",
    "\n",
    "#Merging onto analysis\n",
    "df_analysis = pd.merge(left = df_analysis, right = temp[['date', 'ID', 't']], left_on =['date', 'ID'], right_on = ['date', 'ID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop sector variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "criteria = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(['sector_management_staff', 'sector_trade_service', 'sector_sales_communication', 'sector_teaching', 'sector_social_health', 'sector_other'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform relevant columns to abs change exept those with M_ and ID_, date and t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_actual'] = df_analysis.groupby(['ID'])['target_actual'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagged variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB! As this is Quarterly data one year lag is 4 quarters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['target_4_lag'] = df_analysis.groupby(['ID'])['target_actual'].shift(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 3 month (1 quarter) lag of jobrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1q_lag = ['jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_industry_craft','sector_office_finance']\n",
    "\n",
    "for colname in columns_1q_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropping some GT variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping some GT's\n",
    "drop_list = ['GT_SE_6', 'GT_SE_8', 'GT_SE_10', 'GT_SE_11', 'GT_DK_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.drop(drop_list, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new variables with 1 month lag of GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_1q_lag = ['GT_SE_0', 'GT_SE_1', 'GT_SE_2', 'GT_SE_5', 'GT_SE_7', 'GT_SE_9']\n",
    "for colname in columns_1q_lag:\n",
    "    df_analysis[str(colname + '_1_lag')] = df_analysis.groupby(['ID'])[colname].shift(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Month dummies for season effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['quarter'] = pd.DatetimeIndex(df_analysis['date']).month.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummies from categorial variables - remember to drop the reference category (done after change is constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.get_dummies(df_analysis, prefix=['ID','Q'], prefix_sep='_', columns=['ID', 'quarter']).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2019-07-01 00:00:00')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2008-04-01 00:00:00')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynominal features - To be deleted later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis = add_poly_terms(df = df_analysis, \n",
    "#                            poly_columns = ['target_actual', 'GT_0', 'GT_1', 'GT_2', 'GT_3', 'GT_4', 'GT_5', 'GT_6', 'GT_7', 'GT_8', 'GT_9', 'GT_10', 'GT_11', 'GT_12', 'GT_13', 'GT_14', 'GT_15', 'GT_16', 'GT_17', 'GT_18', 'GT_19', 'target_lag', 'jobs', 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_analysis.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interaction terms by regions and all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # relevant interaction variables\n",
    "# interaction_1 = ['target_lag', 'target_4_lag'] \n",
    "# # 'sector_information_technology', 'sector_engineering_technology', 'sector_management_staff', 'sector_trade_service', 'sector_industry_craft', 'sector_sales_communication', 'sector_teaching', 'sector_office_finance', 'sector_social_health', 'sector_other'\n",
    "\n",
    "# # get list of all ID area \n",
    "# interaction_2 = [item for item in df_analysis if item.startswith('ID_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for var1 in interaction_1:\n",
    "#     for var2 in interaction_2:\n",
    "#         name = var1 + \"*\" + var2\n",
    "#         df_analysis[name] = pd.Series(df_analysis[var1] * df_analysis[var2], name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop variables to not end up in dummytrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis.drop(['ID_Blekinge', 'Q_1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_analysis.drop(interaction_1, axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To Quarter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis['date'] = df_analysis.set_index('date').index.to_period('Q')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2008Q2', 'Q-DEC')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2019Q3', 'Q-DEC')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis['date'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset for period to match DK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_analysis[df_analysis.date >= '01-01-2008']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 11\n",
    "testsize = 1\n",
    "valsize = 1\n",
    "rolling_window = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3510\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "#range(1, X_train[1].shape[1] +1)\n",
    "# Number of trees in random forest\n",
    "n_estimators = [*range(50, 500, 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', \"sqrt\"] # Consider whether this should be set to 'auto as PCA should do its job'\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 3, 5]\n",
    "# Method of selecting samples for training each tree\n",
    "# https://gdcoder.com/random-forest-regression-model-explained-in-depth-part-2-python-code-snippet-using-sklearn/\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'max_features': max_features,\n",
    "    'min_samples_split': min_samples_split,\n",
    "     'min_samples_leaf':min_samples_leaf\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tuning params for window: 100%|███████████████████████████████████████████████████| 34/34 [13:07:53<00:00, 1390.39s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'randomforest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_noint.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing stored results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_noint.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:10<00:00,  3.28it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'randomforest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/randomforest/results_final_noint.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1698456092407148"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n",
    "\n",
    "- Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_val[win] = sc.transform(X_val[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter space - random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24300\n"
     ]
    }
   ],
   "source": [
    "# Website https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "# parameter specification\n",
    "n_components = [0.9]\n",
    "\n",
    "\n",
    "colsample_bytree = [0.3, 0.5, 0.7, 0.9, 1]#is the subsample ratio of columns when constructing each tree. Subsampling will occur once in every boosting iteration. This number ranges from 0 to 1.\n",
    "#learning_rate is the step size shrinkage and is used to prevent overfitting. This number ranges from 0 to 1.\n",
    "# Maximum number of levels in tree\n",
    "# First try: colsample_bytree = [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "max_depth = [*range(3, 11, 1), *range(20, 100, 20)]\n",
    "\n",
    "#max_depth = [*range(10, 300, 10)]\n",
    "# first try max_depth = [*range(50, 500, 20)]\n",
    "\n",
    "n_estimators = [*range(50, 500, 10)]#is the number of boosted trees to fit\n",
    "# first try n_estimators = [*range(50, 500, 20)]\n",
    "\n",
    "gamma = [0]\n",
    "\n",
    "subsample = [0.5, 0.75, 1]\n",
    "\n",
    "min_child_weight = [1, 3, 5]\n",
    "\n",
    "# Create the random grid\n",
    "d = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'colsample_bytree': colsample_bytree,\n",
    "    'gamma' : gamma,\n",
    "    'subsample' : subsample,\n",
    "    'min_child_weight' : min_child_weight\n",
    "    }\n",
    "\n",
    "params = list(d.values())\n",
    "\n",
    "params = list(itertools.product(*params))\n",
    "\n",
    "print(len(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1)\n",
    "params = random.sample(params, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner loop - training hyperparameter on validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On random space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 34/34 [17:06:35<00:00, 1811.64s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mp = tuning_window_mp(X_fit = X_train, y_fit = y_train, X_test = X_val, y_test = y_val, params = params, n_components = n_components, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_noint.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_mp, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outer loop - fitting on train / test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On full sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reloading data and concatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_noint.pickle', 'rb') as handle:\n",
    "    results_opt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test, y_dates = test_train_split_Q(df = df_analysis, window = window, testsize=testsize, valsize = valsize,\n",
    "                                                                  y_col='target_actual', rolling_window = rolling_window, df_output= False, geo_count = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatting val and train\n",
    "for win in X_train.keys():\n",
    "    X_train[win] = np.concatenate((X_train[win], X_val[win])).copy()\n",
    "    y_train[win] = np.concatenate((y_train[win], y_val[win])).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features for each window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for win in X_train.keys():\n",
    "    sc = StandardScaler()\n",
    "    X_train[win] = sc.fit_transform(X_train[win])\n",
    "    X_test[win] = sc.transform(X_test[win])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:04<00:00,  8.24it/s]\n"
     ]
    }
   ],
   "source": [
    "results_final = final_model(inner_results=results_opt, X_fit = X_train, y_fit = y_train, X_test = X_test, y_test = y_test, model_str = 'xgboost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exporting final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results/final/xgboost/results_final_noint.pickle', 'wb') as handle:\n",
    "    pickle.dump(results_final, handle, protocol= pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1158579122612415"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = []\n",
    "for key in results_final.keys():\n",
    "    temp.append(results_final[key]['best_rmse'][1])\n",
    "np.mean(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
